{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNehP553RrML4T3Tjn3/3Hb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unfreaked/codsoft/blob/main/ResumeParser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln7gndxap6id",
        "outputId": "2c071b93-0cb1-4614-e4a7-f6f8a76fa678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.6)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m142.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx PyPDF2 nltk spacy pdf2image pytesseract\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from docx import Document\n",
        "from PyPDF2 import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import io\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load the English language model for spaCy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5zgJtOnyFhl",
        "outputId": "8780bc1a-ddfa-4ffb-8bc0-deed5dcb119a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    pdf_reader = PdfReader(pdf_path)\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = Document(docx_path)\n",
        "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "def extract_text_from_image_pdf(pdf_path):\n",
        "    images = convert_from_path(pdf_path)\n",
        "    text = \"\"\n",
        "    for image in images:\n",
        "        text += pytesseract.image_to_string(image)\n",
        "    return text"
      ],
      "metadata": {
        "id": "T_1EV2m6yJfB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_name(text):\n",
        "    # Use spaCy to find person names\n",
        "    doc = nlp(text)\n",
        "    for entity in doc.ents:\n",
        "        if entity.label_ == \"PERSON\":\n",
        "            return entity.text\n",
        "    return \"Name not found\"\n",
        "\n",
        "def extract_email(text):\n",
        "    email = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
        "    return email[0] if email else \"Email not found\"\n",
        "\n",
        "def extract_phone(text):\n",
        "    phone = re.findall(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]', text)\n",
        "    return phone[0] if phone else \"Phone not found\"\n",
        "\n",
        "def extract_skills(text, skill_keywords):\n",
        "    skills = []\n",
        "    for skill in skill_keywords:\n",
        "        if skill.lower() in text.lower():\n",
        "            skills.append(skill)\n",
        "    return skills if skills else [\"No skills detected\"]\n",
        "\n",
        "def extract_education(text):\n",
        "    education = []\n",
        "    education_keywords = ['bachelor', 'master', 'phd', 'doctorate', 'bs', 'ms', 'mba', 'btech']\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        for word in education_keywords:\n",
        "            if word in sentence.lower():\n",
        "                education.append(sentence)\n",
        "                break\n",
        "\n",
        "    return education if education else [\"Education not specified\"]"
      ],
      "metadata": {
        "id": "BDn3l2reyKsk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_skills = [\n",
        "    'Python', 'Java', 'C++', 'JavaScript', 'HTML', 'CSS',\n",
        "    'SQL', 'Machine Learning', 'Data Analysis', 'Project Management',\n",
        "    'Communication', 'Teamwork', 'Problem Solving', 'AWS', 'Azure',\n",
        "    'Docker', 'Kubernetes', 'React', 'Angular', 'Node.js'\n",
        "]"
      ],
      "metadata": {
        "id": "v6cwz5kcyQhK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR9BxXzk0AxN",
        "outputId": "77ebbdaa-881f-48ff-837c-7b3596cb91b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will prompt you to select a file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ptbYaoPe0VUN",
        "outputId": "190c77ae-7075-495a-bea0-0e906dfa0aec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-236b0317-f0d5-4e6e-9144-2a232123fbb6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-236b0317-f0d5-4e6e-9144-2a232123fbb6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Jake_s_Resume.pdf to Jake_s_Resume (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = list(uploaded.keys())[0]  # Gets the name of your uploaded file\n",
        "print(f\"Uploaded file: {file_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01sTsmvl0npO",
        "outputId": "58611ddd-bbdd-4d8b-d486-fe9c187a2ad2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file: Jake_s_Resume (1).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install python-docx PyPDF2 nltk spacy pdf2image pytesseract\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htU0ZToQ0tuX",
        "outputId": "4a51448f-8976-46f3-cadb-69a2cdc6fe92"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Ign:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 1min 40s (1,868 B/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126109 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.6)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from docx import Document\n",
        "from PyPDF2 import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Text extraction functions\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as f:\n",
        "        pdf_reader = PdfReader(f)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "        return text\n",
        "\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = Document(docx_path)\n",
        "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "def extract_text_from_image_pdf(pdf_path):\n",
        "    images = convert_from_path(pdf_path)\n",
        "    text = \"\"\n",
        "    for image in images:\n",
        "        text += pytesseract.image_to_string(image)\n",
        "    return text\n",
        "\n",
        "# Information extraction functions\n",
        "def extract_name(text):\n",
        "    doc = nlp(text)\n",
        "    for entity in doc.ents:\n",
        "        if entity.label_ == \"PERSON\":\n",
        "            return entity.text\n",
        "    return \"Name not found\"\n",
        "\n",
        "def extract_email(text):\n",
        "    email = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
        "    return email[0] if email else \"Email not found\"\n",
        "\n",
        "def extract_phone(text):\n",
        "    phone = re.findall(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]', text)\n",
        "    return phone[0] if phone else \"Phone not found\"\n",
        "\n",
        "common_skills = ['Python', 'Java', 'C++', 'JavaScript', 'HTML', 'CSS',\n",
        "                'SQL', 'Machine Learning', 'Data Analysis', 'Project Management',\n",
        "                'Communication', 'Teamwork', 'Problem Solving']\n",
        "\n",
        "def extract_skills(text, skill_keywords):\n",
        "    skills = []\n",
        "    for skill in skill_keywords:\n",
        "        if skill.lower() in text.lower():\n",
        "            skills.append(skill)\n",
        "    return skills if skills else [\"No skills detected\"]\n",
        "\n",
        "# Process the uploaded file\n",
        "if file_name.endswith('.pdf'):\n",
        "    try:\n",
        "        text = extract_text_from_pdf(file_name)\n",
        "        if len(text.strip()) < 100:  # If text extraction seems poor\n",
        "            text = extract_text_from_image_pdf(file_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        text = extract_text_from_image_pdf(file_name)\n",
        "elif file_name.endswith('.docx'):\n",
        "    text = extract_text_from_docx(file_name)\n",
        "else:\n",
        "    text = \"Unsupported file format\"\n",
        "\n",
        "# Extract and display information\n",
        "if text and not text.startswith(\"Unsupported\"):\n",
        "    print(\"\\n=== Resume Information ===\")\n",
        "    print(f\"Name: {extract_name(text)}\")\n",
        "    print(f\"Email: {extract_email(text)}\")\n",
        "    print(f\"Phone: {extract_phone(text)}\")\n",
        "    print(\"Skills:\", \", \".join(extract_skills(text, common_skills)))\n",
        "else:\n",
        "    print(\"Failed to process the resume file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxyn1pX21SNC",
        "outputId": "c3b35d58-31df-479d-8845-24b057ab7886"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Resume Information ===\n",
            "Name: Jake Ryan\n",
            "Email: jake@su.edu\n",
            "Phone: 123-456-7890\n",
            "Skills: Python, Java, C++, JavaScript, HTML, CSS, SQL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Create a dictionary with extracted data\n",
        "resume_data = {\n",
        "    \"name\": extract_name(text),\n",
        "    \"email\": extract_email(text),\n",
        "    \"phone\": extract_phone(text),\n",
        "    \"skills\": extract_skills(text, common_skills),\n",
        "}\n",
        "\n",
        "# Save to JSON file\n",
        "with open(\"parsed_resume.json\", \"w\") as f:\n",
        "    json.dump(resume_data, f, indent=4)\n",
        "\n",
        "print(\"Data saved to 'parsed_resume.json'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNNGyUcc1WaT",
        "outputId": "f55f36d7-1268-4570-aaed-29738d6a2d1b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to 'parsed_resume.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "skills = extract_skills(text, common_skills)\n",
        "plt.barh(range(len(skills)), skills)\n",
        "plt.title(\"Skills Found in Resume\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "cLcPNFS_12W1",
        "outputId": "8d40481a-6788-4b9a-81df-b6a4fb11a407"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload multiple files\n",
        "uploaded = files.upload()\n",
        "all_resumes = {}\n",
        "\n",
        "for file_name in uploaded.keys():\n",
        "    if file_name.endswith('.pdf'):\n",
        "        text = extract_text_from_pdf(file_name)\n",
        "    elif file_name.endswith('.docx'):\n",
        "        text = extract_text_from_docx(file_name)\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    all_resumes[file_name] = {\n",
        "        \"name\": extract_name(text),\n",
        "        \"skills\": extract_skills(text, common_skills)\n",
        "    }\n",
        "\n",
        "print(\"Comparison Results:\")\n",
        "for name, data in all_resumes.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Skills: {', '.join(data['skills'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "nJPxY8Gq173k",
        "outputId": "1a923a34-39c7-4384-918d-a7050ed38829"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9cd877ae-66c3-4ed6-adfd-c09e86e9329c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9cd877ae-66c3-4ed6-adfd-c09e86e9329c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ritik_s_resume (1).pdf to ritik_s_resume (1).pdf\n",
            "Comparison Results:\n",
            "\n",
            "ritik_s_resume (1).pdf:\n",
            "Skills: Python, Java, C++, JavaScript, HTML, CSS, SQL, Machine Learning, Data Analysis, Problem Solving\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_experience(text):\n",
        "    doc = nlp(text)\n",
        "    experience = []\n",
        "    for sent in doc.sents:\n",
        "        if \"experience\" in sent.text.lower() or \"worked\" in sent.text.lower():\n",
        "            experience.append(sent.text)\n",
        "    return experience if experience else [\"Experience not specified\"]\n",
        "\n",
        "# Add to resume_data\n",
        "resume_data[\"experience\"] = extract_experience(text)\n",
        "resume_data[\"education\"] = extract_education(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "5hlsCAbn2HYZ",
        "outputId": "5477607b-7c3d-477b-8596-f001f5fcba7e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ce8a8c532b78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Add to resume_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresume_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"experience\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresume_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"education\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_education\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-bc145dff4551>\u001b[0m in \u001b[0;36mextract_education\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0meducation_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bachelor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'master'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'phd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'doctorate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mba'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'btech'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meducation_keywords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')  # Additional useful data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3O0Jk9w2XUk",
        "outputId": "0bd841d0-efc9-41f3-c4e4-002f384b10ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_education_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    education = []\n",
        "    education_keywords = ['bachelor', 'master', 'phd', 'ms', 'bs', 'mba']\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        sent_text = sent.text.lower()\n",
        "        if any(edu_word in sent_text for edu_word in education_keywords):\n",
        "            # Find organizations (universities) in the same sentence\n",
        "            orgs = [ent.text for ent in sent.ents if ent.label_ == \"ORG\"]\n",
        "            if orgs:\n",
        "                education.append(f\"{sent.text.strip()} ({', '.join(orgs)})\")\n",
        "            else:\n",
        "                education.append(sent.text.strip())\n",
        "\n",
        "    return education if education else [\"Education information not found\"]"
      ],
      "metadata": {
        "id": "uy1d6Evb2Yu2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After extracting text...\n",
        "resume_data = {\n",
        "    \"name\": extract_name(text),\n",
        "    \"email\": extract_email(text),\n",
        "    \"phone\": extract_phone(text),\n",
        "    \"skills\": extract_skills(text, common_skills),\n",
        "    \"education\": extract_education_spacy(text),  # Using the spaCy version\n",
        "    \"experience\": extract_experience(text)\n",
        "}\n",
        "\n",
        "print(\"\\n=== Extracted Education ===\")\n",
        "for edu in resume_data[\"education\"]:\n",
        "    print(f\"- {edu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWLuM4Kc2myw",
        "outputId": "a98002e6-e7cc-4744-dbad-a88edc8ee03c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Extracted Education ===\n",
            "- Ritik Ranjan\n",
            "+91 9142599632 |28ritikranjan@gmail.com |http://linkedin.com/in/ritik-ranjan-372710244 |\n",
            "Education\n",
            "KIIT University Bhubaneswar, Odisha\n",
            "Bachelor of Technology in Computer Science Sept. 2022 - Aug. 2026\n",
            "Mount Assisi School Bhagalpur, Bihar\n",
            "ICSE(10th) and ISC(12th Science) (Education, KIIT University Bhubaneswar, Mount Assisi School Bhagalpur)\n",
            "- May 2020 and May 2022\n",
            "Experience (Virtual Internships)\n",
            "Machine Learning Intern Aug 2024 – Sep 2024\n",
            "CodSoft Virtual\n",
            "•Developed and implemented machine learning models for SMS spam detection and credit card fraud detection,\n",
            "leveraging data preprocessing, feature engineering, and classification algorithms to enhance accuracy and efficiency. (SMS)\n",
            "- Full Stack Developer Virual Internship Oct 2024 - Dec 2024\n",
            "AICTE Virtual Internship by Eduskill\n",
            "•Gained practical experience in building web applications using Python, integrating frontend and back-end\n",
            "components\n",
            "Artificial Intelligence and Machine Learning Intern Jan 2025 – Mar 2025\n",
            "AICTE Virtual Internship by India Edu Program Google for Developers\n",
            "•Acquired foundational knowledge of core AI/ML concepts and enhanced problem solving skills through hands on\n",
            "project\n",
            "Projects\n",
            "Water Quality Analysis |Python, Scikit-Learn, XGBoost April 2025\n",
            "•TheVoting Classifier , which combined the strengths of XGBoost and Random Forest, achieved the highest\n",
            "precision score of 0.69 , proving the effectiveness of ensemble learning\n",
            "•Focused on implementing the XGBoost algorithm and tuning hyperparameters\n",
            "•Handled data preprocessing, exploratory data analysis, and implemented Random Forest classifier\n",
            "SMS Spam Detection |Python, NLP, Random Forest, Neural Networks Aug 2024 - Sep 2024\n",
            "•Built an SMS spam classifier using Random Forest andNeural Networks , achieving 95.16% accuracy and\n",
            "100% precision on a dataset of 5,500+ messages\n",
            "•Implemented NLP techniques (tokenization, TF-IDF, stopword removal) to enhance text feature extraction and\n",
            "improve classification performance. (Artificial Intelligence and Machine Learning Intern, India Edu Program Google for, AI/ML, Scikit-Learn, XGBoost, Random Forest, XGBoost, Random Forest, NLP, Random Forest, SMS, Random Forest andNeural Networks, NLP)\n",
            "- Cloud-Deployed E-Commerce Website |Node.js, React.js, AWS, MySQL, Git Mar 2025 - May 2025\n",
            "•Developed and deployed a fully functional e-commerce website supporting 500+ products, user accounts, shopping\n",
            "cart, and payment gateway integration using Stripe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced name extraction using multiple methods\n",
        "def extract_name(text):\n",
        "    # Method 1: spaCy's entity recognition\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            return ent.text\n",
        "\n",
        "    # Method 2: Pattern matching (looking for title case names at beginning)\n",
        "    name_match = re.search(r'^([A-Z][a-z]+(?:\\s[A-Z][a-z]+)+)', text)\n",
        "    if name_match:\n",
        "        return name_match.group(1)\n",
        "\n",
        "    return \"Name not found\"\n",
        "\n",
        "# Enhanced phone number extraction\n",
        "def extract_phone(text):\n",
        "    phone_regex = r'(?:(?:\\+?(\\d{1,3}))?[-. (]*(?:\\d{3})[-. )]*(?:\\d{3})[-. ]*(?:\\d{4})(?: *(?:x|ext|extension)\\.? *(\\d+))?'\n",
        "    phones = re.findall(phone_regex, text)\n",
        "    return [''.join(phone) for phone in phones] if phones else [\"Phone not found\"]"
      ],
      "metadata": {
        "id": "NCKwjhvj3H1E"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_sections(text):\n",
        "    section_headings = {\n",
        "        'experience': ['experience', 'work history', 'employment'],\n",
        "        'education': ['education', 'academic background'],\n",
        "        'skills': ['skills', 'technical skills', 'competencies'],\n",
        "        'projects': ['projects', 'personal projects']\n",
        "    }\n",
        "\n",
        "    sections = {}\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        line_lower = line.strip().lower()\n",
        "        for section, keywords in section_headings.items():\n",
        "            if any(keyword in line_lower for keyword in keywords):\n",
        "                # Get content until next section or end\n",
        "                content = []\n",
        "                for next_line in lines[i+1:]:\n",
        "                    if next_line.strip() and not any(keyword in next_line.lower() for keyword in section_headings.keys()):\n",
        "                        content.append(next_line.strip())\n",
        "                    else:\n",
        "                        break\n",
        "                sections[section] = '\\n'.join(content)\n",
        "                break\n",
        "\n",
        "    return sections"
      ],
      "metadata": {
        "id": "rzbZmg0y3JDb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_full_resume(text):\n",
        "    sections = detect_sections(text)\n",
        "\n",
        "    return {\n",
        "        \"personal_info\": {\n",
        "            \"name\": extract_name(text),\n",
        "            \"email\": extract_email(text),\n",
        "            \"phone\": extract_phone(text)\n",
        "        },\n",
        "        \"education\": extract_education_spacy(text),\n",
        "        \"experience\": sections.get('experience', \"No experience section found\"),\n",
        "        \"skills\": {\n",
        "            \"technical\": extract_skills(text, technical_skills),\n",
        "            \"soft\": extract_skills(text, soft_skills)\n",
        "        },\n",
        "        \"projects\": sections.get('projects', \"No projects section found\"),\n",
        "        \"raw_sections\": sections\n",
        "    }"
      ],
      "metadata": {
        "id": "brwTqmVY3MT4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_extraction(parsed_data, ground_truth):\n",
        "    scores = {}\n",
        "    for field in ['name', 'email', 'phone']:\n",
        "        scores[field] = 1 if parsed_data['personal_info'][field] == ground_truth[field] else 0\n",
        "\n",
        "    # For list fields (skills, education)\n",
        "    for field in ['skills', 'education']:\n",
        "        extracted = set(x.lower() for x in parsed_data[field])\n",
        "        actual = set(x.lower() for x in ground_truth[field])\n",
        "        scores[field] = len(extracted & actual) / len(actual) if actual else 1\n",
        "\n",
        "    return {**scores, \"overall\": sum(scores.values())/len(scores)}"
      ],
      "metadata": {
        "id": "81cSVLIT3NXQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_resume(data, format='json'):\n",
        "    if format == 'json':\n",
        "        import json\n",
        "        return json.dumps(data, indent=2)\n",
        "    elif format == 'csv':\n",
        "        import csv\n",
        "        output = io.StringIO()\n",
        "        writer = csv.writer(output)\n",
        "        writer.writerow(['Field', 'Value'])\n",
        "        for section, content in data.items():\n",
        "            if isinstance(content, dict):\n",
        "                for k, v in content.items():\n",
        "                    writer.writerow([f\"{section}.{k}\", str(v)])\n",
        "            else:\n",
        "                writer.writerow([section, str(content)])\n",
        "        return output.getvalue()\n",
        "    elif format == 'html':\n",
        "        from jinja2 import Template\n",
        "        template = Template('''\n",
        "        <html><body>\n",
        "        <h1>{{ personal_info.name }}</h1>\n",
        "        <h2>Contact</h2>\n",
        "        <p>Email: {{ personal_info.email }}</p>\n",
        "        <!-- More HTML template -->\n",
        "        </body></html>''')\n",
        "        return template.render(**data)"
      ],
      "metadata": {
        "id": "1RJ7-WFB3QPp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_resume_folder(folder_path):\n",
        "    results = []\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(('.pdf', '.docx')):\n",
        "            try:\n",
        "                if file.endswith('.pdf'):\n",
        "                    text = extract_text_from_pdf(os.path.join(folder_path, file))\n",
        "                else:\n",
        "                    text = extract_text_from_docx(os.path.join(folder_path, file))\n",
        "\n",
        "                results.append({\n",
        "                    \"filename\": file,\n",
        "                    \"data\": parse_full_resume(text),\n",
        "                    \"status\": \"success\"\n",
        "                })\n",
        "            except Exception as e:\n",
        "                results.append({\n",
        "                    \"filename\": file,\n",
        "                    \"error\": str(e),\n",
        "                    \"status\": \"failed\"\n",
        "                })\n",
        "    return results"
      ],
      "metadata": {
        "id": "6RYLt21l3SXh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with SQLite\n",
        "import sqlite3\n",
        "\n",
        "def save_to_db(resume_data):\n",
        "    conn = sqlite3.connect('resumes.db')\n",
        "    c = conn.cursor()\n",
        "    c.execute('''CREATE TABLE IF NOT EXISTS resumes\n",
        "                 (name text, email text, skills text, education text)''')\n",
        "    c.execute(\"INSERT INTO resumes VALUES (?,?,?,?)\",\n",
        "              (resume_data['personal_info']['name'],\n",
        "               resume_data['personal_info']['email'],\n",
        "               ','.join(resume_data['skills']['technical']),\n",
        "               '\\n'.join(resume_data['education'])))\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "iaQZbnpO3TcU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a model to classify resume sections\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def train_section_classifier(labeled_data):\n",
        "    # labeled_data = [{\"text\": \"Education\\nMS in CS...\", \"label\": \"education\"}, ...]\n",
        "    texts = [x[\"text\"] for x in labeled_data]\n",
        "    labels = [x[\"label\"] for x in labeled_data]\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, labels)\n",
        "\n",
        "    return vectorizer, model"
      ],
      "metadata": {
        "id": "xRU41f9H3VtJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FastAPI example\n",
        "from fastapi import FastAPI, UploadFile\n",
        "from fastapi.responses import JSONResponse\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/parse-resume\")\n",
        "async def parse_resume(file: UploadFile):\n",
        "    contents = await file.read()\n",
        "    # Add processing logic here\n",
        "    return JSONResponse(parse_full_resume(contents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "eXU_m20Y3XtY",
        "outputId": "73bb5f71-287e-475a-bf32-09e8f705ba3b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fastapi'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-03dc255ca5e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FastAPI example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastAPI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUploadFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJSONResponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastapi'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn python-multipart\n",
        "!pip install python-docx PyPDF2 pdf2image pytesseract spacy nltk\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnkF8HCP3ry4",
        "outputId": "df8f74b6-0496-4591-9253-917d6dd4061b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, python-multipart, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 python-multipart-0.0.20 starlette-0.46.2 uvicorn-0.34.2\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "import io\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "from docx import Document\n",
        "from PyPDF2 import PdfReader\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Initialize NLP model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "app = FastAPI(title=\"Resume Parser API\")\n",
        "\n",
        "# Text extraction functions\n",
        "async def extract_text_from_pdf(file: UploadFile):\n",
        "    pdf_reader = PdfReader(io.BytesIO(await file.read()))\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "async def extract_text_from_docx(file: UploadFile):\n",
        "    doc = Document(io.BytesIO(await file.read()))\n",
        "    return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "# Information extraction functions\n",
        "def extract_name(text: str) -> str:\n",
        "    doc = nlp(text)\n",
        "    for entity in doc.ents:\n",
        "        if entity.label_ == \"PERSON\":\n",
        "            return entity.text\n",
        "    return \"Name not found\"\n",
        "\n",
        "def extract_email(text: str) -> str:\n",
        "    email = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
        "    return email[0] if email else \"Email not found\"\n",
        "\n",
        "@app.post(\"/parse-resume\", response_model=Dict[str, Any])\n",
        "async def parse_resume(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        if file.filename.endswith('.pdf'):\n",
        "            text = await extract_text_from_pdf(file)\n",
        "        elif file.filename.endswith('.docx'):\n",
        "            text = await extract_text_from_docx(file)\n",
        "        else:\n",
        "            raise HTTPException(status_code=400, detail=\"Unsupported file format\")\n",
        "\n",
        "        result = {\n",
        "            \"filename\": file.filename,\n",
        "            \"name\": extract_name(text),\n",
        "            \"email\": extract_email(text),\n",
        "            \"success\": True\n",
        "        }\n",
        "        return JSONResponse(content=result)\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ],
      "metadata": {
        "id": "0aUNJThb3yqx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x6nIuEO4uld",
        "outputId": "6818de3f-c513-4499-e25f-5f3e2d60594f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.9-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqnSs9-P5NXy",
        "outputId": "c233bea5-0b31-4659-b058-fa367e04c965"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# Required for running in notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set your ngrok authtoken (replace 'YOUR_AUTHTOKEN' with your actual token)\n",
        "ngrok.set_auth_token(\"2xl1GEWUFL4jBHceqKm1zhreUpi_L9Fi7L3A948DA6Hux5x3\")\n",
        "\n",
        "# Start the server\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "uvicorn.run(app, host='0.0.0.0', port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4STJ_hz--7tw",
        "outputId": "931d68b4-8ab9-4028-b82f-c167daa6e22a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://a140-34-125-38-184.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [669]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2409:40e4:1341:8709:c77:b76d:19bc:cc43:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2409:40e4:1341:8709:c77:b76d:19bc:cc43:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [669]\n"
          ]
        }
      ]
    }
  ]
}